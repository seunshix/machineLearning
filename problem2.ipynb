{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"problem2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYA+f07V9fjMCrubckyXWF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Ygce9Szj8Pha","executionInfo":{"status":"ok","timestamp":1647224327064,"user_tz":420,"elapsed":1164,"user":{"displayName":"Adebimpe Adeniran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj95AwLMT3mGAAwIxHHZSnGPnd8rMxTX7l5liW2voM=s64","userId":"11448934457515704599"}}},"source":["# reinforcement learning, action, reward, robot\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from matplotlib.table import Table\n","\n","size = 5\n","a = [0, 1]\n","aPrime = [4, 1]\n","b = [0, 3]\n","bPrime = [2, 3]\n","discount = 0.50\n","\n","actions = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]\n","actionArrowsS=[ '←', '↑', '→', '↓']\n","actionProb = 0.25\n","\n","\n","def actionValue(state, action):\n","    if state == a:\n","        return aPrime, 10\n","    if state == b:\n","        return bPrime, 5\n","\n","    next_state = (np.array(state) + action).tolist()\n","    x, y = next_state\n","    if x < 0 or x >= size or y < 0 or y >= size:\n","        reward = -1.0\n","        next_state = state\n","    else:\n","        reward = 0\n","    return next_state, reward\n","\n","\n","def draw_image(image):\n","    fig, ax = plt.subplots()\n","    ax.set_axis_off()\n","    tb = Table(ax, bbox=[0, 0, 1, 1])\n","    nrows, ncols = image.shape\n","    width, height = 1.0 / ncols, 1.0 / nrows\n","    for (i, j), val in np.ndenumerate(image):\n","     \n","        tb.add_cell(i, j, width, height, text=val,\n","                    loc='center', facecolor='white')\n","        \n","    for i in range(len(image)):\n","        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n","                    edgecolor='none', facecolor='none')\n","        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n","                    edgecolor='none', facecolor='none')\n","\n","    ax.add_table(tb)\n","\n","def draw_policy(optimal_values):\n","    fig, ax = plt.subplots()\n","    ax.set_axis_off()\n","    tb = Table(ax, bbox=[0, 0, 1, 1])\n","\n","    nrows, ncols = optimal_values.shape\n","    width, height = 1.0 / ncols, 1.0 / nrows\n","\n","   \n","    for (i, j), val in np.ndenumerate(optimal_values):\n","        next_vals=[]\n","        for action in actions:\n","            next_state, _ = actionValue([i, j], action)\n","            next_vals.append(optimal_values[next_state[0],next_state[1]])\n","\n","        best_actions=np.where(next_vals == np.max(next_vals))[0]\n","        val=''\n","        for ba in best_actions:\n","            val+=actionArrowsS[ba]        \n","        tb.add_cell(i, j, width, height, text=val,\n","                loc='center', facecolor='white')\n","\n","    # Row and column labels...\n","    for i in range(len(optimal_values)):\n","        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n","                    edgecolor='none', facecolor='none')\n","        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n","                   edgecolor='none', facecolor='none')\n","\n","    ax.add_table(tb)\n","\n","\n","def figure_3_2_linear_system():\n","  \n","    A = -1 * np.eye(size * size)\n","    b = np.zeros(size * size)\n","    for i in range(size):\n","        for j in range(size):\n","            s = [i, j]  # current state\n","            index_s = np.ravel_multi_index(s, (size, size))\n","            for a in actions:\n","                s_, r = actionValue(s, a)\n","                index_s_ = np.ravel_multi_index(s_, (size, size))\n","\n","                A[index_s, index_s_] += actionProb * discount\n","                b[index_s] -= actionProb * r\n","\n","    x = np.linalg.solve(A, b)\n","    draw_image(np.round(x.reshape(size, size), decimals=2))\n","    plt.savefig('a.png')\n","    plt.close()\n","\n","def figure_3_5():\n","    value = np.zeros((size, size))\n","    while True:\n","        # keep iteration until convergence\n","        new_value = np.zeros_like(value)\n","        for i in range(size):\n","            for j in range(size):\n","                values = []\n","                for action in actions:\n","                    (next_i, next_j), reward = actionValue([i, j], action)\n","                    # value iteration\n","                    values.append(reward + discount * value[next_i, next_j])\n","                new_value[i, j] = np.max(values)\n","        if np.sum(np.abs(new_value - value)) < 1e-4:\n","            draw_image(np.round(new_value, decimals=2))\n","            plt.savefig('b.png')\n","            plt.close()\n","            draw_policy(new_value)\n","            plt.savefig('c.png')\n","            plt.close()\n","            break\n","        value = new_value\n","\n","\n","\n","figure_3_2_linear_system()\n","figure_3_5()"],"execution_count":1,"outputs":[]}]}